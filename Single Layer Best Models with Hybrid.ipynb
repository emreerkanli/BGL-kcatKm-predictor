{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c763adb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\memre\\anaconda3\\lib\\site-packages\\Bio\\SubsMat\\__init__.py:126: BiopythonDeprecationWarning: Bio.SubsMat has been deprecated, and we intend to remove it in a future release of Biopython. As an alternative, please consider using Bio.Align.substitution_matrices as a replacement, and contact the Biopython developers if you still need the Bio.SubsMat module.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from Bio import pairwise2\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "\n",
    "\n",
    "from Bio import AlignIO\n",
    "from Bio import SeqIO\n",
    "from Bio.Align.Applications import MuscleCommandline\n",
    "from Bio.Align import AlignInfo\n",
    "import pandascharm as pc\n",
    "\n",
    "from Bio.SubsMat.MatrixInfo import blosum62\n",
    "from Bio.SubsMat.MatrixInfo import blosum45\n",
    "\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81279ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Function to save model\n",
    "def save_model(model, model_name):\n",
    "    joblib.dump(model, f'{model_name}.joblib')\n",
    "\n",
    "# Function to load model\n",
    "def load_model(model_name):\n",
    "    return joblib.load(f'{model_name}.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49c2f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeoutlier_col(df,cols):\n",
    "    Q1 = df[cols].quantile(0.25)\n",
    "    Q3 = df[cols].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df_out = df[~((df[[cols]] < (Q1 - 1.5 * IQR)) |(df[[cols]] > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b20b1645",
   "metadata": {},
   "outputs": [],
   "source": [
    "blosum62.update(((b,a),val) for (a,b),val in list(blosum62.items()))\n",
    "blosum45.update(((b,a),val) for (a,b),val in list(blosum45.items()))\n",
    "\n",
    "def score_pairwise(seq1, seq2, matrix, gap_s, gap_e, gap = True):\n",
    "    for A,B in zip(seq1, seq2):\n",
    "        diag = ('-'==A) or ('-'==B)\n",
    "        yield (gap_e if gap else gap_s) if diag else matrix[(A,B)]\n",
    "        gap = diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66fd8433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding With Temperature\n",
    "\n",
    "def encode_temp(encoding, output, df_clean, aln, esm1b ,key = None):\n",
    "    \n",
    "    df_clean = df_clean.set_index('Index')\n",
    "    \n",
    "    ClustalAlign = AlignIO.read(aln, 'fasta')\n",
    "    summary_align = AlignInfo.SummaryInfo(ClustalAlign )\n",
    "    dframe = pc.from_bioalignment(ClustalAlign).transpose()\n",
    "    sequences = dframe.loc[df_clean.index]\n",
    "    \n",
    "    y = df_clean['Log' + output]\n",
    "\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    \n",
    "    \n",
    "    if encoding == 'One-Hot-Encoder':\n",
    "\n",
    "        one_hot = OneHotEncoder()\n",
    "        encoded = one_hot.fit(sequences)\n",
    "        sequences_encoded = encoded.transform(sequences).toarray()\n",
    "        X = np.concatenate((sequences_encoded,df_clean[['Reaction Temperature']]), axis =1)\n",
    "\n",
    "        \n",
    "           \n",
    "    if encoding == 'Bag-of-Words':\n",
    "\n",
    "        X = pd.DataFrame([ProteinAnalysis(i).count_amino_acids() for i in df_clean['Sequence']])\n",
    "        X = np.concatenate((X,df_clean[['Reaction Temperature']]), axis =1)\n",
    "\n",
    "        \n",
    "    if encoding == 'bigram':\n",
    "        \n",
    "        X = df_clean['Sequence']\n",
    "\n",
    "        example = df_clean['Sequence'][0]\n",
    "        lst = ['E','G','L','Y','T','H','R','A','C','D','P','I','F','N','K','S','V','M','W','Q']\n",
    "        all_dct = {}\n",
    "        key = []\n",
    "        for i in lst:\n",
    "            for j in lst:\n",
    "                st = i+j\n",
    "                all_dct[st] = []\n",
    "\n",
    "        for example, id in zip(X,range(len(X))):\n",
    "\n",
    "            temp = list(example)\n",
    "            temp_dct = dict.fromkeys(all_dct.keys(),0)\n",
    "            for k in range(len(temp)-1):\n",
    "                try:\n",
    "                    check = temp[k] + temp[k+1]\n",
    "                    temp_dct[check] += 1\n",
    "                except:\n",
    "                    pass\n",
    "            for key, value in temp_dct.items():\n",
    "                all_dct[key].append(value)\n",
    "                \n",
    "                \n",
    "        X = pd.DataFrame.from_dict(all_dct).set_index(df_clean.index)\n",
    "        X['Reaction Temperature'] = df_clean['Reaction Temperature']\n",
    "\n",
    "    \n",
    "    if encoding == 'trigram':\n",
    "        \n",
    "        X = df_clean['Sequence']\n",
    "\n",
    "        example = df_clean['Sequence'][0]\n",
    "        lst = ['E','G','L','Y','T','H','R','A','C','D','P','I','F','N','K','S','V','M','W','Q']\n",
    "        all_dct = {}\n",
    "        key = []\n",
    "        for i in lst:\n",
    "            for j in lst:\n",
    "                for k in lst:\n",
    "                    st = i+j+k\n",
    "                    all_dct[st] = []\n",
    "\n",
    "        for example, id in zip(X,range(len(X))):\n",
    "\n",
    "            temp = list(example)\n",
    "            temp_dct = dict.fromkeys(all_dct.keys(),0)\n",
    "            for k in range(len(temp)-2):\n",
    "                try:\n",
    "                    check = temp[k] + temp[k+1]+temp[k+2]\n",
    "                    temp_dct[check] += 1\n",
    "                except:\n",
    "                    pass\n",
    "            for key, value in temp_dct.items():\n",
    "                all_dct[key].append(value)\n",
    "                \n",
    "        X = pd.DataFrame.from_dict(all_dct).set_index(df_clean.index)\n",
    "        X['Reaction Temperature'] = df_clean['Reaction Temperature']\n",
    "\n",
    "        \n",
    "        \n",
    "    if encoding == 'quadrogram':\n",
    "        \n",
    "        X = df_clean['Sequence']\n",
    "\n",
    "        example = df_clean['Sequence'][0]\n",
    "        lst = ['E','G','L','Y','T','H','R','A','C','D','P','I','F','N','K','S','V','M','W','Q']\n",
    "        all_dct = {}\n",
    "        key = []\n",
    "        for i in lst:\n",
    "            for j in lst:\n",
    "                for k in lst:\n",
    "                    for l in lst:\n",
    "                        st = i+j+k+l\n",
    "                        all_dct[st] = []\n",
    "\n",
    "        for example, id in zip(X,range(len(X))):\n",
    "\n",
    "            temp = list(example)\n",
    "            temp_dct = dict.fromkeys(all_dct.keys(),0)\n",
    "            for k in range(len(temp)-3):\n",
    "                try:\n",
    "                    check = temp[k] + temp[k+1]+temp[k+2]+temp[k+3]\n",
    "                    temp_dct[check] += 1\n",
    "                except:\n",
    "                    pass\n",
    "            for key, value in temp_dct.items():\n",
    "                all_dct[key].append(value)\n",
    "                \n",
    "        X = pd.DataFrame.from_dict(all_dct).set_index(df_clean.index)\n",
    "        X['Reaction Temperature'] = df_clean['Reaction Temperature']\n",
    "\n",
    "\n",
    "    if encoding == 'BLOSUM62':\n",
    "\n",
    "        n = len(sequences)\n",
    "        enc_seq = np.zeros((n,n))\n",
    "\n",
    "        i = 0\n",
    "\n",
    "        for a in list(sequences.index):\n",
    "            j = 0\n",
    "            for b in list(sequences.index):\n",
    "                enc_seq[i,j] = sum(score_pairwise(sequences.loc[a], sequences.loc[b], blosum62, -5, -1))\n",
    "                j += 1\n",
    "            i += 1\n",
    "\n",
    "        X = np.concatenate((enc_seq[:,:-6],df_clean[['Reaction Temperature']]), axis =1)\n",
    "\n",
    "        \n",
    "        \n",
    "    if encoding == 'BLOSUM45':\n",
    "        \n",
    "        n = len(sequences)\n",
    "        enc_seq = np.zeros((n,n))\n",
    "\n",
    "        i = 0\n",
    "\n",
    "        for a in list(sequences.index):\n",
    "            j = 0\n",
    "            for b in list(sequences.index):\n",
    "                enc_seq[i,j] = sum(score_pairwise(sequences.loc[a], sequences.loc[b], blosum45, -5, -1))\n",
    "                j += 1\n",
    "            i += 1\n",
    "\n",
    "        X = np.concatenate((enc_seq[:,:-6],df_clean[['Reaction Temperature']]), axis =1)\n",
    "\n",
    "        \n",
    "    if encoding == 'ESM1b':\n",
    "        \n",
    "\n",
    "        encoded = esm1b.loc[df_clean.index]\n",
    "        encoded['Reaction Temperature'] = df_clean['Reaction Temperature']\n",
    "        X = np.array(encoded)\n",
    "\n",
    "    X = np.array(X)  \n",
    "    scaler.fit(X)\n",
    "    X_scaled = scaler.transform(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "084d2be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [['Random Forest', 'BLOSUM62'], ['SVM', 'ESM1b'], \n",
    "              ['SVM', 'bigram'],['Random Forest', 'ESM1b']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2edd7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "enzyme = 'betaGlucosidasewithMutants'\n",
    "\n",
    "df = pd.read_excel('betaGlucosidasewithMutantsOptimumTemperatureHybrid.xlsx')\n",
    "\n",
    "output = 'pNP-Glc kcat/Km (1/smM)'\n",
    "aln = 'betaGlucosidasewithMutantsHybrid.fa'\n",
    "\n",
    "x = datetime.datetime.now()\n",
    "date = str(x.year)+str(x.month)+str(x.day)\n",
    "\n",
    "df['Log'+output] = np.log10(df[output])\n",
    "esm1b = pd.read_excel('betaGlucosidasewithMutantsHybridESM1b_embeddings.xlsx', index_col = 0)\n",
    "\n",
    "random_states = [202 , 1, 42, 101, 2022,5 , 10, 22, 1995, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59e479bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df_predicted_activity = []    \n",
    "list_df_train_predicted_activity  = []    \n",
    "list_df_hybrid_predicted_activity  = []  \n",
    "    \n",
    "for state in random_states:    \n",
    "    \n",
    "    predicted_activity_list = []\n",
    "    predicted_train_activity_list = []\n",
    "    predicted_hybrid_activity_list = []\n",
    "    model_name_list = []\n",
    "\n",
    "    df_ancestor = df.tail(6)\n",
    "\n",
    "    df_clean = removeoutlier_col(df.iloc[:-6],'Log' + output).reset_index()\n",
    "\n",
    "    # Create a mapping of unique sequences to unique codes\n",
    "    sequence_to_code = {seq: f\"ENZYME_{i+1}\" for i, seq in enumerate(df_clean['Sequence'].unique())}\n",
    "\n",
    "    # Map these codes to a new column in the DataFrame using .loc\n",
    "    df_clean.loc[:, 'Sequence Code'] = df_clean['Sequence'].map(sequence_to_code)\n",
    "\n",
    "    \n",
    "    \n",
    "    splitter = GroupShuffleSplit(test_size=.20, n_splits=10, random_state = state)\n",
    "    split = splitter.split(df_clean, groups=df_clean['Sequence Code'])\n",
    "    train_inds, val_inds = next(split)\n",
    "    \n",
    "    \n",
    "    df_clean = pd.concat([df_clean, df_ancestor], ignore_index=True)\n",
    "    hybrid_inds = df_clean.tail(6).index\n",
    "    y = df_clean['Log'+output]\n",
    "    \n",
    "    X_BLOSUM62 = encode_temp('BLOSUM62', output, df_clean, aln, esm1b ,key = None)\n",
    "    X_ESM1b = encode_temp('ESM1b', output, df_clean, aln, esm1b ,key = None)\n",
    "    X_bigram = encode_temp('bigram', output, df_clean, aln, esm1b ,key = None)\n",
    "    \n",
    "    sequence_dictionary = {'ESM1b': X_ESM1b,'BLOSUM62': X_BLOSUM62,'bigram' : X_bigram}\n",
    "    \n",
    "\n",
    "    for model_algorithm, model_method in model_list:\n",
    "\n",
    "        X = sequence_dictionary[model_method]\n",
    "        \n",
    "        model_name = f'{model_method}_{model_algorithm}_singleLayer_state_{state}'\n",
    "        model = load_model(model_name)\n",
    "        \n",
    "        \n",
    "        if model_algorithm in ['LASSO', 'SVM', 'Neural Network']:\n",
    "            scaler = preprocessing.StandardScaler()\n",
    "            scaler.fit(X[:-6])\n",
    "            X = scaler.transform(X)\n",
    "        \n",
    "        prediction_yval = model.predict(X[val_inds])\n",
    "        train_pred = model.predict(X[train_inds])\n",
    "        hybrid_pred = model.predict(X[hybrid_inds])\n",
    "        \n",
    "        predicted_activity_list.append(prediction_yval)\n",
    "        predicted_train_activity_list.append(train_pred)\n",
    "        predicted_hybrid_activity_list.append(hybrid_pred)\n",
    "\n",
    "        model_name_list.append(model_name)\n",
    "\n",
    "\n",
    "    df_predicted_activity = pd.DataFrame(predicted_activity_list, index = model_name_list , \n",
    "                                         columns=df_clean.loc[val_inds]['Index'].values).transpose()\n",
    "    df_predicted_activity['y_val'] = y[val_inds].values\n",
    "    \n",
    "    \n",
    "    df_train_predicted_activity = pd.DataFrame(predicted_train_activity_list, index = model_name_list, \n",
    "                                               columns=df_clean.loc[train_inds]['Index'].values ).transpose()\n",
    "    df_train_predicted_activity['y_val'] = y[train_inds].values\n",
    "    \n",
    "    \n",
    "    df_hybrid_predicted_activity = pd.DataFrame(predicted_hybrid_activity_list, index = model_name_list, \n",
    "                                               columns=df_clean.loc[hybrid_inds]['Index'].values ).transpose()\n",
    "    df_hybrid_predicted_activity['y_val'] = y[hybrid_inds].values\n",
    "    \n",
    "    list_df_predicted_activity.append(df_predicted_activity)\n",
    "    list_df_train_predicted_activity.append(df_train_predicted_activity)\n",
    "    list_df_hybrid_predicted_activity.append(df_hybrid_predicted_activity)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e13445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['Trial 1', 'Trial 2', 'Trial 3', 'Trial 4', 'Trial 5', 'Trial 6', 'Trial 7', 'Trial 8', 'Trial 9', 'Trial 10']\n",
    "\n",
    "writer=pd.ExcelWriter(r\"C:\\Users\\memre\\Desktop\\Research\\Predicting Enzyme Properties Based on Various Organisms\\Code\\Temperature Profile Prediction\\20250122 SingleLayerbetaGlucosidase-y_val.xlsx\")\n",
    "_ = [A.to_excel(writer,sheet_name=\"{0}\".format(names[i])) for i, A in enumerate(list_df_predicted_activity)]\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdc3373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer=pd.ExcelWriter(r\"C:\\Users\\memre\\Desktop\\Research\\Predicting Enzyme Properties Based on Various Organisms\\Code\\Temperature Profile Prediction\\20250122 SingleLayerbetaGlucosidase-y_train.xlsx\")\n",
    "_ = [A.to_excel(writer,sheet_name=\"{0}\".format(names[i])) for i, A in enumerate(list_df_train_predicted_activity)]\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21be49e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer=pd.ExcelWriter(r\"C:\\Users\\memre\\Desktop\\Research\\Predicting Enzyme Properties Based on Various Organisms\\Code\\Temperature Profile Prediction\\20250122 SingleLayerbetaGlucosidase-y_hybrid.xlsx\")\n",
    "_ = [A.to_excel(writer,sheet_name=\"{0}\".format(names[i])) for i, A in enumerate(list_df_hybrid_predicted_activity)]\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f40211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "from datetime import datetime\n",
    "current_date = datetime.now().strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebbb3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['Trial 1', 'Trial 2', 'Trial 3', 'Trial 4', 'Trial 5', 'Trial 6', 'Trial 7', 'Trial 8', 'Trial 9', 'Trial 10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa10c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df_predicted_activity = pd.read_excel('20241125 SingleLayerbetaGlucosidase-y_val.xlsx', index_col =0,\n",
    "                          sheet_name =names)\n",
    "list_df_train_predicted_activity = pd.read_excel('20241125 SingleLayerbetaGlucosidase-y_train.xlsx', index_col =0,\n",
    "                          sheet_name =names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492bb02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining Training and Validation\n",
    "list_df_entire_predicted_activity = {}\n",
    "\n",
    "for i in names:  # Iterate over each split\n",
    "    combined_df = pd.concat([list_df_train_predicted_activity[i], list_df_predicted_activity [i]])  # Keep original indices\n",
    "    list_df_entire_predicted_activity[i] = combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4483b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = [list_df_predicted_activity , list_df_train_predicted_activity, list_df_entire_predicted_activity]\n",
    "dataset_types = [\"Validation\", \"Training\", \"Entire\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6ab86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric lists for storing results\n",
    "list_R2_df = []\n",
    "list_RMSE_df = []\n",
    "list_MAE_df = []\n",
    "list_PCC_df = []\n",
    "list_pValue_df = []\n",
    "\n",
    "i =1\n",
    "\n",
    "# Process each dataset (validation, training, entire)\n",
    "for data, dataset_type in zip(data_set, dataset_types):\n",
    "    list_R2, list_RMSE, list_MAE, list_PCC, list_pValue = [], [], [], [], []\n",
    "\n",
    "    # Loop through each split in the dataset dictionary\n",
    "    for state_key in data:\n",
    "        state = data[state_key]  # Access the DataFrame\n",
    "\n",
    "        r2_list, rmse_list, mae_list, pcc_list, pvalue_list = [], [], [], [], []\n",
    "\n",
    "        # Calculate metrics for each model in the dataframe\n",
    "        for model in state.columns[:-1]:  # Assuming 'y_val' is the last column\n",
    "            r2 = r2_score(state['y_val'], state[model])\n",
    "            rmse = mean_squared_error(state['y_val'], state[model], squared=False)\n",
    "            mae = mean_absolute_error(state['y_val'], state[model])\n",
    "            pcc, pValue = pearsonr(state['y_val'], state[model])\n",
    "\n",
    "            r2_list.append(r2)\n",
    "            rmse_list.append(rmse)\n",
    "            mae_list.append(mae)\n",
    "            pcc_list.append(pcc)\n",
    "            pvalue_list.append(pValue)\n",
    "\n",
    "        # Append results for this split\n",
    "        list_R2.append(r2_list)\n",
    "        list_RMSE.append(rmse_list)\n",
    "        list_MAE.append(mae_list)\n",
    "        list_PCC.append(pcc_list)\n",
    "        list_pValue.append(pvalue_list)\n",
    "        \n",
    "    column_names = state.columns[:-1]\n",
    "\n",
    "    # Convert metric lists to DataFrames for each split\n",
    "    Results_R2 = pd.DataFrame(list_R2, columns=column_names, index = names)\n",
    "    Results_RMSE = pd.DataFrame(list_RMSE, columns=column_names,index = names)\n",
    "    Results_MAE = pd.DataFrame(list_MAE, columns=column_names, index = names)\n",
    "    Results_PCC = pd.DataFrame(list_PCC, columns=column_names, index = names)\n",
    "    Results_pValue = pd.DataFrame(list_pValue, columns=column_names, index = names)\n",
    "\n",
    "    # Append each metric's result DataFrame\n",
    "    list_R2_df.append(Results_R2)\n",
    "    list_RMSE_df.append(Results_RMSE)\n",
    "    list_MAE_df.append(Results_MAE)\n",
    "    list_PCC_df.append(Results_PCC)\n",
    "    list_pValue_df.append(Results_pValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a247d488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write metrics to Excel file with specified structure\n",
    "def write_metrics_to_excel(filename, metric_dfs, metric_name):\n",
    "    with pd.ExcelWriter(filename) as writer:\n",
    "        # Initialize empty DataFrames for validation, training, and entire dataset results\n",
    "\n",
    "        # Select the first four columns for Validation\n",
    "        validation_df = metric_dfs[0]\n",
    "\n",
    "        # Select the next four columns for Training\n",
    "        training_df = metric_dfs[1]\n",
    "\n",
    "        # Select the last four columns for Entire\n",
    "        entire_df =metric_dfs[2]\n",
    "\n",
    "        # Write each dataset type to a separate sheet\n",
    "        validation_df.to_excel(writer, sheet_name=\"Validation\", index=False)\n",
    "        training_df.to_excel(writer, sheet_name=\"Training\", index=False)\n",
    "        entire_df.to_excel(writer, sheet_name=\"Entire\", index=False)\n",
    "\n",
    "        # Calculate the average across the 10 trials for each dataset type\n",
    "        avg_validation = validation_df.mean(axis=0)\n",
    "        avg_training = training_df.mean(axis=0)\n",
    "        avg_entire = entire_df.mean(axis=0)\n",
    "\n",
    "        \n",
    "        model_names =  validation_df.columns\n",
    "        # Create DataFrames for average values\n",
    "        avg_validation_df = pd.DataFrame(avg_validation, columns=['Average'], index = model_names)\n",
    "        avg_training_df = pd.DataFrame(avg_training, columns=['Average'], index = model_names)\n",
    "        avg_entire_df = pd.DataFrame(avg_entire, columns=['Average'], index = model_names)\n",
    "\n",
    "        # Write average values to separate sheets\n",
    "        avg_validation_df.to_excel(writer, sheet_name=\"Avg_Validation\")\n",
    "        avg_training_df.to_excel(writer, sheet_name=\"Avg_Training\",)\n",
    "        avg_entire_df.to_excel(writer, sheet_name=\"Avg_Entire\")\n",
    "\n",
    "        # Calculate the standard deviation across the 10 trials for each dataset type\n",
    "        std_validation = validation_df.std(axis=0)\n",
    "        std_training = training_df.std(axis=0)\n",
    "        std_entire = entire_df.std(axis=0)\n",
    "\n",
    "        # Create DataFrames for standard deviation values\n",
    "        std_validation_df = pd.DataFrame(std_validation, columns=['Std_Dev'], index = model_names)\n",
    "        std_training_df = pd.DataFrame(std_training, columns=['Std_Dev'], index = model_names)\n",
    "        std_entire_df = pd.DataFrame(std_entire, columns=['Std_Dev'], index = model_names)\n",
    "\n",
    "        # Write standard deviation values to separate sheets\n",
    "        std_validation_df.to_excel(writer, sheet_name=\"Std_Validation\")\n",
    "        std_training_df.to_excel(writer, sheet_name=\"Std_Training\")\n",
    "        std_entire_df.to_excel(writer, sheet_name=\"Std_Entire\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231e0282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write each metric to separate Excel files with date in the filename\n",
    "for metric_dfs, metric_name in zip(\n",
    "        [list_R2_df, list_RMSE_df, list_MAE_df, list_PCC_df, list_pValue_df],\n",
    "        [\"R2\", \"RMSE\", \"MAE\", \"PCC\", \"pValue\"]):\n",
    "    filename = f\"{current_date}_{metric_name}_singleLayerResults_.xlsx\"\n",
    "    write_metrics_to_excel(filename, metric_dfs, metric_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d108ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
